{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning for the Data Science Challenge "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "@Author  : Hager Oueslati - Data Scientist at Capgemini France \n",
    "@Version : Python 3.6.3\n",
    "This notebook contains all the diffrent steps to clean text Data before starting the analysis \n",
    "All the steps are grouped in one single function called \"text_cleaning\"\n",
    "Each step of the text processing is explained with a title comment\n",
    "NLTK python module (Natural Language Toolkit) provides many functionalities to work with human language data.\n",
    "Many of them were used to build the text cleaning \"engine\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.3\n"
     ]
    }
   ],
   "source": [
    "#return Python version\n",
    "from platform import python_version\n",
    "print (python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "########### Import Libraries ##############\n",
    "###########################################\n",
    "import pandas as pd\n",
    "import os\n",
    "import re         \n",
    "import codecs\n",
    "import string\n",
    "import subprocess \n",
    "import nltk\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from string import digits\n",
    "from nltk.corpus import stopwords\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "############## Import Data ################\n",
    "###########################################\n",
    "#the ppt extracted data\n",
    "data_path = \"C:/Users/houeslat/Documents/DS interne/Challenges/Challenge_v0/results.csv\"\n",
    "pwd = os.getcwd()\n",
    "os.chdir(os.path.dirname(data_path))\n",
    "data = pd.read_csv(os.path.basename(data_path),sep=';')\n",
    "os.chdir(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>Client_overview</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Solution</th>\n",
       "      <th>Benefit</th>\n",
       "      <th>Other details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2083_E_OMV_BI_Landscape_130917_DT.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nissue\\n\\xef\\x82\\xa7\\xef\\x82\\xa7 historically...</td>\n",
       "      <td>\\nsolution\\n\\xef\\x82\\xa7\\xef\\x82\\xa7 focus\\nfo...</td>\n",
       "      <td>\\nbenefits\\n\\nref\\n(da\\nkm\\n(da\\neng\\nnam\\ncli...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2052_E_Oil&amp;Gas_BI_Cost_Savings_Elaboration1310...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nissue\\n\\xef\\x82\\xa7\\xef\\x82\\xa7\\n\\n\\xef\\x82\\...</td>\n",
       "      <td>\\n</td>\n",
       "      <td>\\nsolution\\nbenefits\\nsolution\\n\\xef\\x82\\xa7\\x...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2046_E_2_OMV_BI_ControllingLandscape_130903.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nissue\\n\\xef\\x82\\xa7\\xef\\x82\\xa7 deal\\ndeal w...</td>\n",
       "      <td>\\nsolution\\n\\xef\\x82\\xa7\\xef\\x82\\xa7 developme...</td>\n",
       "      <td>\\nbenefits\\n\\xef\\x82\\xa7\\xef\\x82\\xa7 consisten...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boeing.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s\\nthe boeing enterprise issued an rfp for a s...</td>\n",
       "      <td>s\\n:while boeing is not allowing capgemini to ...</td>\n",
       "      <td>/results\\ncapgemini\\xe2\\x80\\x99s contract has ...</td>\n",
       "      <td>\\n\\nclient name: boeing\\ntop line initiative (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name  Client_overview  \\\n",
       "0              2083_E_OMV_BI_Landscape_130917_DT.pdf              NaN   \n",
       "1  2052_E_Oil&Gas_BI_Cost_Savings_Elaboration1310...              NaN   \n",
       "2    2046_E_2_OMV_BI_ControllingLandscape_130903.pdf              NaN   \n",
       "3                                         Boeing.pdf              NaN   \n",
       "\n",
       "                                               Issue  \\\n",
       "0  \\nissue\\n\\xef\\x82\\xa7\\xef\\x82\\xa7 historically...   \n",
       "1  \\nissue\\n\\xef\\x82\\xa7\\xef\\x82\\xa7\\n\\n\\xef\\x82\\...   \n",
       "2  \\nissue\\n\\xef\\x82\\xa7\\xef\\x82\\xa7 deal\\ndeal w...   \n",
       "3  s\\nthe boeing enterprise issued an rfp for a s...   \n",
       "\n",
       "                                            Solution  \\\n",
       "0  \\nsolution\\n\\xef\\x82\\xa7\\xef\\x82\\xa7 focus\\nfo...   \n",
       "1                                                 \\n   \n",
       "2  \\nsolution\\n\\xef\\x82\\xa7\\xef\\x82\\xa7 developme...   \n",
       "3  s\\n:while boeing is not allowing capgemini to ...   \n",
       "\n",
       "                                             Benefit  \\\n",
       "0  \\nbenefits\\n\\nref\\n(da\\nkm\\n(da\\neng\\nnam\\ncli...   \n",
       "1  \\nsolution\\nbenefits\\nsolution\\n\\xef\\x82\\xa7\\x...   \n",
       "2  \\nbenefits\\n\\xef\\x82\\xa7\\xef\\x82\\xa7 consisten...   \n",
       "3  /results\\ncapgemini\\xe2\\x80\\x99s contract has ...   \n",
       "\n",
       "                                       Other details  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3  \\n\\nclient name: boeing\\ntop line initiative (...  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "##### Define text cleaning function #######\n",
    "###########################################\n",
    "def text_cleaning(text,escape_list=[]):\n",
    "    l=[]\n",
    "    \"\"\"\n",
    "    Text cleaning function:\n",
    "        Input: \n",
    "            -text: a string variable, the text to be cleaned\n",
    "            -escape_list : words not to transform by the cleaning process (only lowcase transformation is needed)  \n",
    "        Output:\n",
    "            -text cleaned and stemmed           \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\" Get stop word list from package\"\"\"\n",
    "    StopWords = list(set(stopwords.words('english')))\n",
    "    \n",
    "    \"\"\" Step 1: Parse html entities\"\"\"\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    \"\"\" Step 2: Decode special caracters\"\"\"\n",
    "    text = text.encode('utf8').decode('unicode_escape')\n",
    "    \n",
    " \n",
    "    \"\"\" Step 3: Tokenise text: spliting text elements with the TreeBankWordTokenizer method\"\"\"\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    tokenz=[','.join(tokenizer.tokenize(mot)) if mot  not in escape_list else mot  for mot in text.split()  ]\n",
    "    \n",
    "    \n",
    "    \"\"\" Step 4: Drop punctuations \"\"\"\n",
    "    tokenz=[re.sub(r'[^\\w\\s]',' ',mot) if mot  not in escape_list else mot  for mot in tokenz  ]\n",
    "    tokenz = ' '.join(tokenz).split()\n",
    "       \n",
    "    \"\"\" Step 5.1: Remove stop words \"\"\"\n",
    "    tokenz=([token for token in tokenz if token not in StopWords])\n",
    "    \n",
    "    \n",
    "    \"\"\" Step 5.2: Delete digits from text \"\"\"\n",
    "    tokenz=([token for token in tokenz if (  (token.isdigit())==False)  ])  \n",
    "\n",
    "    \"\"\" Step 5.3: Remove digits from tokens\"\"\"\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    tokenz=[token.translate(remove_digits)  if token not in  escape_list else token for token in tokenz   ]\n",
    "    \n",
    "    \"\"\" Step 6.1: Lowcase the text\"\"\"\n",
    "    tokenz=([token.lower() for token in tokenz])\n",
    "    \n",
    "    \"\"\" Step 6.2: Stem the text \"\"\"\n",
    "    tokenz=[EnglishStemmer().stem(token) if token not in escape_list else token for token in tokenz ]\n",
    "\n",
    "    \"\"\" Step 6.3: Drop words with one caratcter and proceed last check for stop words after Stemming\"\"\"\n",
    "    tokenz=[token for token in tokenz if (token not in  StopWords and len(token)>1) ]\n",
    "\n",
    "    return ' '.join(tokenz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\nissue\\\\n\\\\xef\\\\x82\\\\xa7\\\\xef\\\\x82\\\\xa7 historically\\\\nhistorically grown\\\\ngrown\\\\ncontrolling\\\\nbi\\\\ncontrolling bi\\\\nlandscape\\\\nlandscape lacking\\\\nlacking aa\\\\nclear\\\\nstrategy\\\\nclear strategy and\\\\nand\\\\npositioning\\\\nof\\\\ntools\\\\npositioning of tools\\\\n\\\\xef\\\\x82\\\\xa7\\\\xef\\\\x82\\\\xa7 data\\\\ndata modeling\\\\nmodeling //\\\\nmaster\\\\nmaster data\\\\ndata\\\\nmaintenance\\\\nmaintenance rights\\\\nrights\\\\ndistributed\\\\nintensively\\\\ndistributed intensively\\\\nacross\\\\nacross the\\\\nthe user\\\\nuser\\\\ncommunity\\\\ncommunity\\\\n\\\\xef\\\\x82\\\\xa7\\\\xef\\\\x82\\\\xa7 no\\\\nno standards\\\\nstandards and\\\\nand\\\\nclear\\\\ncontrol\\\\nfor\\\\nclear control for data\\\\ndata\\\\ndefinitions/\\\\ndefinitions/ structure/\\\\nstructure/\\\\nsources\\\\nsources (e.g.\\\\n(e.g. different\\\\ndifferent\\\\nnaming\\\\nconventions\\\\nnaming conventions\\\\nfor\\\\nfor same\\\\nsame kpis)\\\\nkpis)\\\\n\\\\xef\\\\x82\\\\xa7\\\\xef\\\\x82\\\\xa7 consequently,\\\\nconsequently, high\\\\nhigh\\\\nredundancies\\\\nredundancies in\\\\nin data\\\\ndata\\\\ncubes\\\\nand\\\\ncubes and\\\\ninconsistencies\\\\ninconsistencies in\\\\nin\\\\nnaming\\\\nof\\\\ndata\\\\nlead\\\\nnaming of data lead to\\\\nto\\\\ninefficiencies\\\\nin\\\\ninefficiencies in\\\\nreporting\\\\nreporting\\\\n\\\\nsolution\\\\n'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pick a text example from the ppt data extraction\n",
    "text = data['Issue'][0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'issu histor histor grown grown control bi control bi landscap landscap lack lack aa clear strategi clear strategi posit tool posit tool data data model model master master data data mainten mainten right right distribut intens distribut intens across across user user communiti communiti standard standard clear control clear control data data definit definit structur structur sourc sourc differ differ name convent name convent kpis kpis consequ consequ high high redund redund data data cube cube inconsist inconsist name data lead name data lead ineffici ineffici report report solut'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The output result\n",
    "text_cleaning(text,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'issu histor histor grown grown control bi control bi landscap landscap lack lack aa clear strategy clear strategy posit tool posit tool data data model model master master data data mainten mainten right right distributed intens distributed intens across across user user communiti communiti standard standard clear control clear control data data definit definit structur structur sourc sourc differ differ name convent name convent kpis kpis consequ consequ high high redund redund data data cube cube inconsist inconsist name data lead name data lead inefficiencies inefficiencies report report solut'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adding elements to the escape list\n",
    "text_cleaning(text,[\"distributed\",\"strategy\",\"inefficiencies\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1:  Escape HTML Caracters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\nissue\\\\n\\\\xef\\\\x82\\\\xa7\\\\xef\\\\x82\\\\xa7 historically\\\\nhistorically grown\\\\ngrown\\\\ncontrolling\\\\nbi\\\\ncontrolling bi\\\\nlandscape\\\\nlandscape lacking\\\\nlacking aa\\\\nclear\\\\nstrategy\\\\nclear strategy and\\\\nand\\\\npositioning\\\\nof\\\\ntools\\\\npositioning of tools\\\\n\\\\xef\\\\x82\\\\xa7\\\\xef\\\\x82\\\\xa7 data\\\\ndata modeling\\\\nmodeling //\\\\nmaster\\\\nmaster data\\\\ndata\\\\nmaintenance\\\\nmaintenance rights\\\\nrights\\\\ndistributed\\\\nintensively\\\\ndistributed intensively\\\\nacross\\\\nacross the\\\\nthe user\\\\nuser\\\\ncommunity\\\\ncommunity\\\\n\\\\xef\\\\x82\\\\xa7\\\\xef\\\\x82\\\\xa7 no\\\\nno standards\\\\nstandards and\\\\nand\\\\nclear\\\\ncontrol &lt; &gt; &amp; R2D2 21'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add some htlm entities to the text\n",
    "escape_list = []\n",
    "text = text[:551] + ' &lt; &gt; &amp; R2D2 21'\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\nissue\\\\n\\\\xef\\\\x82\\\\xa7\\\\xef\\\\x82\\\\xa7 historically\\\\nhistorically grown\\\\ngrown\\\\ncontrolling\\\\nbi\\\\ncontrolling bi\\\\nlandscape\\\\nlandscape lacking\\\\nlacking aa\\\\nclear\\\\nstrategy\\\\nclear strategy and\\\\nand\\\\npositioning\\\\nof\\\\ntools\\\\npositioning of tools\\\\n\\\\xef\\\\x82\\\\xa7\\\\xef\\\\x82\\\\xa7 data\\\\ndata modeling\\\\nmodeling //\\\\nmaster\\\\nmaster data\\\\ndata\\\\nmaintenance\\\\nmaintenance rights\\\\nrights\\\\ndistributed\\\\nintensively\\\\ndistributed intensively\\\\nacross\\\\nacross the\\\\nthe user\\\\nuser\\\\ncommunity\\\\ncommunity\\\\n\\\\xef\\\\x82\\\\xa7\\\\xef\\\\x82\\\\xa7 no\\\\nno standards\\\\nstandards and\\\\nand\\\\nclear\\\\ncontrol < > & R2D2 21'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = html.unescape(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Decode encoded Caracters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nissue\\nï\\x82§ï\\x82§ historically\\nhistorically grown\\ngrown\\ncontrolling\\nbi\\ncontrolling bi\\nlandscape\\nlandscape lacking\\nlacking aa\\nclear\\nstrategy\\nclear strategy and\\nand\\npositioning\\nof\\ntools\\npositioning of tools\\nï\\x82§ï\\x82§ data\\ndata modeling\\nmodeling //\\nmaster\\nmaster data\\ndata\\nmaintenance\\nmaintenance rights\\nrights\\ndistributed\\nintensively\\ndistributed intensively\\nacross\\nacross the\\nthe user\\nuser\\ncommunity\\ncommunity\\nï\\x82§ï\\x82§ no\\nno standards\\nstandards and\\nand\\nclear\\ncontrol < > & R2D2 21'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.encode('utf8').decode('unicode_escape')\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Text Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['issue', 'ï\\x82§ï\\x82§', 'historically', 'historically', 'grown', 'grown', 'controlling', 'bi', 'controlling', 'bi', 'landscape', 'landscape', 'lacking', 'lacking', 'aa', 'clear', 'strategy', 'clear', 'strategy', 'and', 'and', 'positioning', 'of', 'tools', 'positioning', 'of', 'tools', 'ï\\x82§ï\\x82§', 'data', 'data', 'modeling', 'modeling', '//', 'master', 'master', 'data', 'data', 'maintenance', 'maintenance', 'rights', 'rights', 'distributed', 'intensively', 'distributed', 'intensively', 'across', 'across', 'the', 'the', 'user', 'user', 'community', 'community', 'ï\\x82§ï\\x82§', 'no', 'no', 'standards', 'standards', 'and', 'and', 'clear', 'control', '<', '>', '&', 'R2D2', '21']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenz=[','.join(tokenizer.tokenize(mot)) if mot  not in escape_list else mot  for mot in text.split()  ]\n",
    "print(tokenz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ã¯ÂÂ§Ã¯ÂÂ§\n"
     ]
    }
   ],
   "source": [
    "print('Ã¯Â\\x82Â§Ã¯Â\\x82Â§')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['issue', 'ï  ï  ', 'historically', 'historically', 'grown', 'grown', 'controlling', 'bi', 'controlling', 'bi', 'landscape', 'landscape', 'lacking', 'lacking', 'aa', 'clear', 'strategy', 'clear', 'strategy', 'and', 'and', 'positioning', 'of', 'tools', 'positioning', 'of', 'tools', 'ï  ï  ', 'data', 'data', 'modeling', 'modeling', '  ', 'master', 'master', 'data', 'data', 'maintenance', 'maintenance', 'rights', 'rights', 'distributed', 'intensively', 'distributed', 'intensively', 'across', 'across', 'the', 'the', 'user', 'user', 'community', 'community', 'ï  ï  ', 'no', 'no', 'standards', 'standards', 'and', 'and', 'clear', 'control', ' ', ' ', ' ', 'R2D2', '21']\n"
     ]
    }
   ],
   "source": [
    "#The regex wqill also get rid of other special caracters\n",
    "tokenz=[re.sub(r'[^\\w\\s]',' ',mot) if mot  not in escape_list else mot  for mot in tokenz  ]\n",
    "print(tokenz)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['issue', 'ï', 'ï', 'historically', 'historically', 'grown', 'grown', 'controlling', 'bi', 'controlling', 'bi', 'landscape', 'landscape', 'lacking', 'lacking', 'aa', 'clear', 'strategy', 'clear', 'strategy', 'and', 'and', 'positioning', 'of', 'tools', 'positioning', 'of', 'tools', 'ï', 'ï', 'data', 'data', 'modeling', 'modeling', 'master', 'master', 'data', 'data', 'maintenance', 'maintenance', 'rights', 'rights', 'distributed', 'intensively', 'distributed', 'intensively', 'across', 'across', 'the', 'the', 'user', 'user', 'community', 'community', 'ï', 'ï', 'no', 'no', 'standards', 'standards', 'and', 'and', 'clear', 'control', 'R2D2', '21']\n"
     ]
    }
   ],
   "source": [
    "#after droping punctuation and special caracters, we need to remove extra white spaces\n",
    "tokenz = ' '.join(tokenz).split()\n",
    "print(tokenz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yourselves', 'she', 'its', 'from', 'while', 'those', 'and', 'been', 'doing', 'd']\n"
     ]
    }
   ],
   "source": [
    "#Get the sto word list from nltk \n",
    "StopWords = list(set(stopwords.words('english')))\n",
    "print(StopWords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['issue', 'ï', 'ï', 'historically', 'historically', 'grown', 'grown', 'controlling', 'bi', 'controlling', 'bi', 'landscape', 'landscape', 'lacking', 'lacking', 'aa', 'clear', 'strategy', 'clear', 'strategy', 'positioning', 'tools', 'positioning', 'tools', 'ï', 'ï', 'data', 'data', 'modeling', 'modeling', 'master', 'master', 'data', 'data', 'maintenance', 'maintenance', 'rights', 'rights', 'distributed', 'intensively', 'distributed', 'intensively', 'across', 'across', 'user', 'user', 'community', 'community', 'ï', 'ï', 'standards', 'standards', 'clear', 'control', 'R2D2', '21']\n"
     ]
    }
   ],
   "source": [
    "tokenz=([token for token in tokenz if token not in StopWords])\n",
    "print(tokenz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['issue', 'ï', 'ï', 'historically', 'historically', 'grown', 'grown', 'controlling', 'bi', 'controlling', 'bi', 'landscape', 'landscape', 'lacking', 'lacking', 'aa', 'clear', 'strategy', 'clear', 'strategy', 'positioning', 'tools', 'positioning', 'tools', 'ï', 'ï', 'data', 'data', 'modeling', 'modeling', 'master', 'master', 'data', 'data', 'maintenance', 'maintenance', 'rights', 'rights', 'distributed', 'intensively', 'distributed', 'intensively', 'across', 'across', 'user', 'user', 'community', 'community', 'ï', 'ï', 'standards', 'standards', 'clear', 'control', 'R2D2']\n"
     ]
    }
   ],
   "source": [
    "#remove digit from text\n",
    "tokenz=([token for token in tokenz if (  (token.isdigit())==False)  ]) \n",
    "print(tokenz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['issue', 'ï', 'ï', 'historically', 'historically', 'grown', 'grown', 'controlling', 'bi', 'controlling', 'bi', 'landscape', 'landscape', 'lacking', 'lacking', 'aa', 'clear', 'strategy', 'clear', 'strategy', 'positioning', 'tools', 'positioning', 'tools', 'ï', 'ï', 'data', 'data', 'modeling', 'modeling', 'master', 'master', 'data', 'data', 'maintenance', 'maintenance', 'rights', 'rights', 'distributed', 'intensively', 'distributed', 'intensively', 'across', 'across', 'user', 'user', 'community', 'community', 'ï', 'ï', 'standards', 'standards', 'clear', 'control', 'RD']\n"
     ]
    }
   ],
   "source": [
    "#remove digit from tokens\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "tokenz = [token.translate(remove_digits)  if token not in  escape_list else token for token in tokenz   ]\n",
    "print(tokenz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: lowcase and Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['issue', 'ï', 'ï', 'historically', 'historically', 'grown', 'grown', 'controlling', 'bi', 'controlling', 'bi', 'landscape', 'landscape', 'lacking', 'lacking', 'aa', 'clear', 'strategy', 'clear', 'strategy', 'positioning', 'tools', 'positioning', 'tools', 'ï', 'ï', 'data', 'data', 'modeling', 'modeling', 'master', 'master', 'data', 'data', 'maintenance', 'maintenance', 'rights', 'rights', 'distributed', 'intensively', 'distributed', 'intensively', 'across', 'across', 'user', 'user', 'community', 'community', 'ï', 'ï', 'standards', 'standards', 'clear', 'control', 'rd']\n"
     ]
    }
   ],
   "source": [
    "#Lowcase the text\n",
    "tokenz=([token.lower() for token in tokenz])\n",
    "print(tokenz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['issu', 'ï', 'ï', 'histor', 'histor', 'grown', 'grown', 'control', 'bi', 'control', 'bi', 'landscap', 'landscap', 'lack', 'lack', 'aa', 'clear', 'strategi', 'clear', 'strategi', 'posit', 'tool', 'posit', 'tool', 'ï', 'ï', 'data', 'data', 'model', 'model', 'master', 'master', 'data', 'data', 'mainten', 'mainten', 'right', 'right', 'distribut', 'intens', 'distribut', 'intens', 'across', 'across', 'user', 'user', 'communiti', 'communiti', 'ï', 'ï', 'standard', 'standard', 'clear', 'control', 'rd']\n"
     ]
    }
   ],
   "source": [
    "#Stem the text \n",
    "tokenz=[EnglishStemmer().stem(token) if token not in escape_list else token for token in tokenz ]\n",
    "print(tokenz)\n",
    "#you can use Lemmatizer instead \n",
    "#see the following article https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['issu', 'histor', 'histor', 'grown', 'grown', 'control', 'bi', 'control', 'bi', 'landscap', 'landscap', 'lack', 'lack', 'aa', 'clear', 'strategi', 'clear', 'strategi', 'posit', 'tool', 'posit', 'tool', 'data', 'data', 'model', 'model', 'master', 'master', 'data', 'data', 'mainten', 'mainten', 'right', 'right', 'distribut', 'intens', 'distribut', 'intens', 'across', 'across', 'user', 'user', 'communiti', 'communiti', 'standard', 'standard', 'clear', 'control', 'rd']\n"
     ]
    }
   ],
   "source": [
    "#Drop words with one caratcter and proceed last check for stop words after Stemming\"\"\"\n",
    "tokenz=[token for token in tokenz if (token not in  StopWords and len(token)>1) ]\n",
    "print(tokenz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'issu histor histor grown grown control bi control bi landscap landscap lack lack aa clear strategi clear strategi posit tool posit tool data data model model master master data data mainten mainten right right distribut intens distribut intens across across user user communiti communiti standard standard clear control rd'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final result \n",
    "#join all stems in one paragraph\n",
    "' '.join(tokenz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
