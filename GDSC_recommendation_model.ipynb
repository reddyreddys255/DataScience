{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nName        : Halloween Search - The Efficient Reference Recommendation Engine\\nDescription : Selects reference ppts based on a score which combines cosine similarity, country match, sector match etc.\\nPurpose     : Created for the Capgemini Global Data Science Challenge IV\\nAuthor:     : Arka Prava Bandyopadhyay\\nTeam        : Team_Halloween\\nDate:       : 21/03/2018\\nVersion:    : 3.33\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Name        : Halloween Search - The Efficient Reference Recommendation Engine\n",
    "Description : Selects reference ppts based on a score which combines cosine similarity, country match, sector match etc.\n",
    "Purpose     : Created for the Capgemini Global Data Science Challenge IV\n",
    "Author:     : Arka Prava Bandyopadhyay\n",
    "Team        : Team_Halloween\n",
    "Date:       : 21/03/2018\n",
    "Version:    : 3.33\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insights And Data Reference Booklet - By Sector.pptx\n",
      "CoxCommunicationsInc_201712_BigData.pptx\n",
      "BMW_201206_BIStrategy_140212.pptx\n",
      "Astellas_201302_QlikView.pptx\n",
      "BayerBusinessServicesGmbH_201712_SAP.pptx\n",
      "TelecomCompany_201702_DataMigrationOfBillingSystem.pptx\n",
      "Bahlsen_201311_SAP_BW.pptx\n",
      "Westpac_201310_Data_Quality.pptx\n",
      "RBS_201106_Data_Migration.pptx\n",
      "GlobalPharmaceuticalCompany_201302_QlikView.pptx\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import html\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "###################################################################################################################\n",
    "########################                  The Sector Dictionary          ##########################################\n",
    "###################################################################################################################\n",
    "\n",
    "sector_dict={'Automotive': ['Automotive','Audi','BMW','Borg','automobile','auto','automobiles'] ,\n",
    "'Electronics & High Tech': ['HTECH' ,'High','Tech','ISOLA','Isola','Electronics'],\n",
    "'Government & Public Sector': ['Public', 'Sector','PS','Railways','Maharashtra','Lufthansa','HMRC','Corning',\n",
    "                               'Amtrak','Rail','Municipal','Anser','Government'],\n",
    "'Consumer Products & Retail': ['CPRDT','Consumer', 'Products', 'Retail','CPRDT','CPRD','Retails','Bumble','Adidas','Mc',\n",
    "                               \"Lowe's\",'Ahold','Unilever','Cargill','Beiersdorf','FMCGGiant','Loves','Nestle','Burberry'],\n",
    "'Healthcare & Life Sciences':['Bio', 'Pharma' , 'Health', 'Life','Pharmaceutical','Pharma','Pharmacitical','Hospital',\n",
    "                              'USPharma','Astellas','Healthcare'],\n",
    "'Financial Services':['Financial','FS'],\n",
    "'Insurance':['Insurance','Allianz','MAAF'],\n",
    "'Banking & Capital Markets':['Bank','Banking','Deutsche'],\n",
    "'Manufacturing & Industrial Products': ['Manufacturing','Ferro','Siemens','Boeing','Meggitt','MALS'],\n",
    "'Natural Resources':['Energy', 'Utilities','Gas','Net','Electric'],\n",
    "'Media & Entertainment': ['Media & Entertainment' ,'Media','Movie','Studio','Netflix','Entertainment'],\n",
    "'Telecoms':['TME','Telecom','Telco','Telefonica','T-Mobile','Cable','Telecoms'],\n",
    "'End User Computing': ['Hydro','SUEZ','GDF']\n",
    "          }\n",
    "###################################################################################################################\n",
    "########################                  The Technology Dictionary with multiple names         ###################\n",
    "###################################################################################################################\n",
    "\n",
    "tech_dict={\n",
    "    'Sap Business Objects': ['Business Objects', 'BO','BusinessObjects'],\n",
    "    'Informatica': ['PowerCenter' ,'Informatica'],\n",
    "    'Business Warehouse':['bw','business warehouse'],\n",
    "    'Data Warehouse':['dw','Data Warehouse','datawarehouse'],\n",
    "    'Business Intelligence':['bi','Business Intelligence'],\n",
    "    'Microsoft':['ms','microsoft'],\n",
    "    'Master Data Management':['mdm','Master Data Management'],\n",
    "    'Customer Relationship Management':['crm','Customer Relationship Management'],\n",
    "    'Cloud':['cloud','aws','azure','google cloud'],\n",
    "    'Big Data':['bigdata','big data','hadoop','cloudera'],\n",
    "    'General Data Protection Regulation':['gdpr','Data Protection Regulation'],\n",
    "    'Artificial Intelligence':['Artificial Intelligence','ai'],\n",
    "    'enterprise data warehouse':['data warehouse','edw','datawarehouse'],\n",
    "    \n",
    "}\n",
    "\n",
    "###########################################################################\n",
    "########## The COUNTRY List taken from pycountry module #############\n",
    "###########################################################################\n",
    "\n",
    "country_name = ['Aruba', 'Afghanistan', 'Angola', 'Anguilla', 'Åland Islands', 'Albania', 'Andorra', 'United Arab Emirates', 'Argentina', 'Armenia', 'American Samoa', 'Antarctica', 'French Southern Territories', 'Antigua and Barbuda', 'Australia', 'Austria', 'Azerbaijan', 'Burundi', 'Belgium', 'Benin', 'Bonaire, Sint Eustatius and Saba', 'Burkina Faso', 'Bangladesh', 'Bulgaria', 'Bahrain', 'Bahamas', 'Bosnia and Herzegovina', 'Saint Barthélemy', 'Belarus', 'Belize', 'Bermuda', 'Bolivia, Plurinational State of', 'Brazil', 'Barbados', 'Brunei Darussalam', 'Bhutan', 'Bouvet Island', 'Botswana', 'Central African Republic', 'Canada', 'Cocos (Keeling) Islands', 'Switzerland', 'Chile', 'China', \"Côte d'Ivoire\", 'Cameroon', 'Congo, The Democratic Republic of the', 'Congo', 'Cook Islands', 'Colombia', 'Comoros', 'Cabo Verde', 'Costa Rica', 'Cuba', 'Curaçao', 'Christmas Island', 'Cayman Islands', 'Cyprus', 'Czechia', 'Germany', 'Djibouti', 'Dominica', 'Denmark', 'Dominican Republic', 'Algeria', 'Ecuador', 'Egypt', 'Eritrea', 'Western Sahara', 'Spain', 'Estonia', 'Ethiopia', 'Finland', 'Fiji', 'Falkland Islands (Malvinas)', 'France', 'Faroe Islands', 'Micronesia, Federated States of', 'Gabon', 'United Kingdom', 'Georgia', 'Guernsey', 'Ghana', 'Gibraltar', 'Guinea', 'Guadeloupe', 'Gambia', 'Guinea-Bissau', 'Equatorial Guinea', 'Greece', 'Grenada', 'Greenland', 'Guatemala', 'French Guiana', 'Guam', 'Guyana', 'Hong Kong', 'Heard Island and McDonald Islands', 'Honduras', 'Croatia', 'Haiti', 'Hungary', 'Indonesia', 'Isle of Man', 'India', 'British Indian Ocean Territory', 'Ireland', 'Iran, Islamic Republic of', 'Iraq', 'Iceland', 'Israel', 'Italy', 'Jamaica', 'Jersey', 'Jordan', 'Japan', 'Kazakhstan', 'Kenya', 'Kyrgyzstan', 'Cambodia', 'Kiribati', 'Saint Kitts and Nevis', 'Korea, Republic of', 'Kuwait', \"Lao People's Democratic Republic\", 'Lebanon', 'Liberia', 'Libya', 'Saint Lucia', 'Liechtenstein', 'Sri Lanka', 'Lesotho', 'Lithuania', 'Luxembourg', 'Latvia', 'Macao', 'Saint Martin (French part)', 'Morocco', 'Monaco', 'Moldova, Republic of', 'Madagascar', 'Maldives', 'Mexico', 'Marshall Islands', 'Macedonia, Republic of', 'Mali', 'Malta', 'Myanmar', 'Montenegro', 'Mongolia', 'Northern Mariana Islands', 'Mozambique', 'Mauritania', 'Montserrat', 'Martinique', 'Mauritius', 'Malawi', 'Malaysia', 'Mayotte', 'Namibia', 'New Caledonia', 'Niger', 'Norfolk Island', 'Nigeria', 'Nicaragua', 'Niue', 'Netherlands', 'Norway', 'Nepal', 'Nauru', 'New Zealand', 'Oman', 'Pakistan', 'Panama', 'Pitcairn', 'Peru', 'Philippines', 'Palau', 'Papua New Guinea', 'Poland', 'Puerto Rico', \"Korea, Democratic People's Republic of\", 'Portugal', 'Paraguay', 'Palestine, State of', 'French Polynesia', 'Qatar', 'Réunion', 'Romania', 'Russian Federation', 'Rwanda', 'Saudi Arabia', 'Sudan', 'Senegal', 'Singapore', 'South Georgia and the South Sandwich Islands', 'Saint Helena, Ascension and Tristan da Cunha', 'Svalbard and Jan Mayen', 'Solomon Islands', 'Sierra Leone', 'El Salvador', 'San Marino', 'Somalia', 'Saint Pierre and Miquelon', 'Serbia', 'South Sudan', 'Sao Tome and Principe', 'Suriname', 'Slovakia', 'Slovenia', 'Sweden', 'Swaziland', 'Sint Maarten (Dutch part)', 'Seychelles', 'Syrian Arab Republic', 'Turks and Caicos Islands', 'Chad', 'Togo', 'Thailand', 'Tajikistan', 'Tokelau', 'Turkmenistan', 'Timor-Leste', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Tuvalu', 'Taiwan, Province of China', 'Tanzania, United Republic of', 'Uganda', 'Ukraine', 'United States Minor Outlying Islands', 'Uruguay', 'United States', 'Uzbekistan', 'Holy See (Vatican City State)', 'Saint Vincent and the Grenadines', 'Venezuela, Bolivarian Republic of', 'Virgin Islands, British', 'Virgin Islands, U.S.', 'Viet Nam', 'Vanuatu', 'Wallis and Futuna', 'Samoa', 'Yemen', 'South Africa', 'Zambia', 'Zimbabwe']\n",
    "###########################################################################\n",
    "################ Stopwords List from NLTK stopwords #######################\n",
    "###########################################################################\n",
    "\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
    "\n",
    "def findWholeWord(w):\n",
    "    return re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE).search\n",
    "\n",
    "def search(query,df1):\n",
    "    #################################################################################################\n",
    "    ####         the sales team base their choices mainly on 3 criterions :                      ####\n",
    "    ####         - Same need (BI analytics, Big Data architecture, MDM...)                       ####\n",
    "    ####         - Same technology (SAP, Cognos, Cloudera...)                                    ####\n",
    "    ####         - Same business sector (Life sciences, Automotive, media & entertainment...)    ####\n",
    "    #################################################################################################\n",
    "\n",
    "    ########################################################################\n",
    "    ###############      The Search Query         ##########################\n",
    "    ########################################################################\n",
    "\n",
    "    search_query=query.lower()\n",
    "    \n",
    "    #########################################################################\n",
    "    #######         Deleting stopwords from search query    #################\n",
    "    #########################################################################\n",
    "    \n",
    "    search_query=' '.join(([i for i in search_query.split() if i not in stopwords]))\n",
    "    \n",
    "    ########################################################################\n",
    "    #########           Getting Country from Query   ########################\n",
    "    ########################################################################\n",
    "    country=list()\n",
    "    df1['query_country']=0\n",
    "    for i in country_name:\n",
    "        if search_query.find(i.lower())!=-1:\n",
    "            country.append(i)\n",
    "    if ('us' in search_query.split()) or ('usa' in search_query.split()) or ('na' in search_query.split()) or ('america' in search_query.split()):\n",
    "        country.append('US')\n",
    "    if 'uk' in search_query.split():\n",
    "        country.append('UK')\n",
    "    if len(country) != 0:\n",
    "        for c,i in enumerate(country):\n",
    "            if i == 'United States':\n",
    "                country[c]= 'US'\n",
    "            elif country == 'United Kingdom':\n",
    "                country[c] = 'UK'\n",
    "        for count,i in enumerate(df1['country']):\n",
    "            for name_country in country:\n",
    "                if i.find(name_country)!=-1:\n",
    "                    df1.loc[count,('query_country')]=1\n",
    "    \n",
    "    # Replacing Words Those words which can bring false cosine similarity\n",
    "    \n",
    "    for i in ['na']:\n",
    "        search_query=search_query.replace(i,'')\n",
    "\n",
    "    ########################################################################\n",
    "    #########           Getting Sector from Query   ########################\n",
    "    ########################################################################\n",
    "    query_sector=''\n",
    "    for key,value in sector_dict.items():\n",
    "        for i in value:\n",
    "            if i.lower() in search_query.split():\n",
    "                query_sector=key\n",
    "    ########################################################################\n",
    "    ############    Marking The Sector   ####################################\n",
    "    ########################################################################\n",
    "    df1['query_sector']=0\n",
    "    for count,i in enumerate(df1['Sector']):\n",
    "        if query_sector =='Financial Services':\n",
    "            if(df1.loc[count,('Sector')]=='Banking & Capital Markets' and df1.loc[count,('Sector')]!=''):\n",
    "                df1.loc[count,('query_sector')]+=1\n",
    "            if(df1.loc[count,('Sector')]=='Insurance' and df1.loc[count,('Sector')]!=''):\n",
    "                df1.loc[count,('query_sector')]+=1\n",
    "        if(df1.loc[count,('Sector')]==query_sector and df1.loc[count,('Sector')]!=''):\n",
    "            df1.loc[count,('query_sector')]+=1\n",
    "\n",
    "    ########################################################################\n",
    "    ############    Marking The Year    ####################################\n",
    "    ########################################################################\n",
    "    df1['query_year']=0\n",
    "    try:\n",
    "        query_year=re.findall(r'\\d{2,4}',search_query)[0]\n",
    "        if query_year!='':\n",
    "            for count,i in enumerate(df1['year']):\n",
    "                if query_year ==df1.loc[count,('year')]:\n",
    "                        df1.loc[count,('query_year')]+=1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #######################################################################\n",
    "    ############    Marking The Name   ####################################\n",
    "    #######################################################################\n",
    "    df1['name_match']=0\n",
    "    for i in search_query.split():\n",
    "        if i not in ['company','ai']: #### Not in the sector values\n",
    "            for count,j in enumerate(df1['Client_name']): ##### Checking for the name part\n",
    "                if j.lower().find(i)>-1:\n",
    "                    #df1['Match_tech'][count]+=1\n",
    "                    if df1.loc[count,('name_match')]==0:\n",
    "                        df1.loc[count,('name_match')]+=1\n",
    "                    else:\n",
    "                        df1.loc[count,('name_match')]+=0.1\n",
    "            for count,j in enumerate(df1['Technology Name']):\n",
    "                if j.lower().find(i)>-1:\n",
    "                    #df1['Match_tech'][count]+=1\n",
    "                    if df1.loc[count,('name_match')]==0:\n",
    "                        df1.loc[count,('name_match')]+=.2\n",
    "                    else:\n",
    "                        df1.loc[count,('name_match')]+=0.1\n",
    "    #######################################################################\n",
    "    ####################  Marking From Tech Corpus ########################\n",
    "    #######################################################################\n",
    "    for key,value in tech_dict.items():\n",
    "        for i in value:\n",
    "            if findWholeWord(i.lower())(search_query):\n",
    "                for i in tech_dict.get(key):\n",
    "                    if not findWholeWord(i.lower())(search_query):\n",
    "                        search_query+= ' '+i.lower()\n",
    "                break\n",
    "\n",
    "    #######################################################################\n",
    "    ############    Marking The Tech   ####################################\n",
    "    #######################################################################\n",
    "    df1['Match_tech']=0\n",
    "    #Techcorpus--Technologies Used        \n",
    "    TechCorpus=''\n",
    "    for i in df1['Tech']:\n",
    "        TechCorpus+=i\n",
    "    for i in search_query.split():\n",
    "        if i not in ['data']: #To prevent data to be assigned to teradata or datastage etc.\n",
    "            if TechCorpus.lower().find(i)>-1:\n",
    "                for count,j in enumerate(df1['Tech']):\n",
    "                    if j.lower().find(i)>-1:\n",
    "                        #df1['Match_tech'][count]+=1\n",
    "                        if df1.loc[count,('Match_tech')]==0:\n",
    "                            df1.loc[count,('Match_tech')]+=1\n",
    "                        else:\n",
    "                            df1.loc[count,('Match_tech')]+=0.1\n",
    "    # Replacing Words Those words which can bring false cosine similarity\n",
    "    \n",
    "    for i in ['informatica']:\n",
    "        search_query=search_query.replace(i,'')\n",
    "    ###########################################################################################################\n",
    "    ###########################################################################################################\n",
    "    ###############    Main Recommander System By TF-IDF and Cosine Similarity    #############################\n",
    "    ###########################################################################################################\n",
    "    ###########################################################################################################\n",
    "    vec = TfidfVectorizer()\n",
    "    doclist_tfidf = vec.fit_transform(df1['Full_text']).toarray()\n",
    "    query_tfidf = vec.transform([' '.join([EnglishStemmer().stem(token) for token in search_query.split()])]).toarray()\n",
    "    rec_idx = cosine_similarity(doclist_tfidf, query_tfidf)\n",
    "    df1['cos_sim']=rec_idx\n",
    "    \n",
    "    ##########################\n",
    "    ###### SCORE #############\n",
    "    ##########################\n",
    "\n",
    "    df1['score']=0\n",
    "    for i in range(len(df1)):\n",
    "        df1.loc[i,('score')] = df1.loc[i,('cos_sim')]*1 + df1.loc[i,('Match_tech')]*.20 +df1.loc[i,('query_sector')] * 0.40 +df1.loc[i,('query_year')] * 0.20+ df1.loc[i,('name_match')]*0.30+df1.loc[i,('query_country')]*0.20\n",
    "        if (df1.loc[i,('Match_tech')]>=1 and df1.loc[i,('query_sector')]==1):\n",
    "            df1.loc[i,('score')] +=0.1\n",
    "        if (df1.loc[i,('name_match')]>=1 and df1.loc[i,('query_sector')]==1):\n",
    "            df1.loc[i,('score')] +=0.2\n",
    "    #To ensure top similarity ppts get better weightage \n",
    "    top4_similarity=df1.sort_values(['cos_sim'],ascending=False).reset_index().loc[:3,'file_name_actual']\n",
    "    for i in range(len(df1)):\n",
    "        if df1.loc[i,('file_name_actual')] in list(top4_similarity):\n",
    "            if query_sector == '':\n",
    "                #For those unfortunate ppts which doesn't have features extracted properly\n",
    "                # All these if elses are to prevent non sector matches to gain more numbers\n",
    "                if df1.loc[i,('cos_sim')]>0.1:\n",
    "                    df1.loc[i,('score')] = df1.loc[i,('score')] + df1.loc[i,('cos_sim')]/df1.loc[i,('score')] \n",
    "                elif (0.05<df1.loc[i,('cos_sim')]<0.1):\n",
    "                    df1.loc[i,('score')] = df1.loc[i,('score')] + df1.loc[i,('cos_sim')]/df1.loc[i,('score')] * 0.5\n",
    "            else:\n",
    "                if (df1.loc[i,('cos_sim')]>0.1):\n",
    "                    if(df1.loc[i,('Sector')]==query_sector):\n",
    "                        df1.loc[i,('score')] = df1.loc[i,('score')] + df1.loc[i,('cos_sim')]/df1.loc[i,('score')]\n",
    "                    else:\n",
    "                        df1.loc[i,('score')] = df1.loc[i,('score')] + df1.loc[i,('cos_sim')]/df1.loc[i,('score')]*0.50\n",
    "                elif (0.05<df1.loc[i,('cos_sim')]<0.1):\n",
    "                    if(df1.loc[i,('Sector')]==query_sector):\n",
    "                        df1.loc[i,('score')] = df1.loc[i,('score')] + df1.loc[i,('cos_sim')]/df1.loc[i,('score')]*0.50\n",
    "                    else:\n",
    "                        df1.loc[i,('score')] = df1.loc[i,('score')] + df1.loc[i,('cos_sim')]/df1.loc[i,('score')]*0.25  \n",
    "    # To prevent models without any similaritirs to feature in final results\n",
    "    for i in range(len(df1)):\n",
    "        if df1.loc[i,('cos_sim')]<0.0000000001 and search_query.find('powercenter')==-1: #to help informatica\n",
    "            df1.loc[i,('score')]=df1.loc[i,('score')]*0.30\n",
    "    return df1[df1['score']>0].sort_values(['score'],ascending=False).loc[:,['file_name_actual']]\n",
    "\n",
    "def main():\n",
    "    ####################################################################################\n",
    "    #########     Calling the main search function and printing the result    ##########\n",
    "    ####################################################################################\n",
    "    df1= pd.read_pickle(\"trained_df.pkl\")\n",
    "    #Reading the Search Query from the command line\n",
    "    search_query = sys.argv[1]\n",
    "    search_result=search(search_query,df1)\n",
    "    # Assigning a random result if \n",
    "    if len(search_result)==0:\n",
    "        #Old Logic#search_result=df1[(df1['text_length']<6000)&(df1['Tech']!='')&(df1['Sector']!='')].sort_values('text_length',ascending=False)[:30].sort_values('year',ascending=False).loc[:,['file_name_actual']]\n",
    "        #This one is based on most frequent ppts for the queries in first evaluation round\n",
    "        for i in ['Insights And Data Reference Booklet - By Sector.pptx','CoxCommunicationsInc_201712_BigData.pptx','BMW_201206_BIStrategy_140212.pptx','Astellas_201302_QlikView.pptx','BayerBusinessServicesGmbH_201712_SAP.pptx','TelecomCompany_201702_DataMigrationOfBillingSystem.pptx','Bahlsen_201311_SAP_BW.pptx','Westpac_201310_Data_Quality.pptx','RBS_201106_Data_Migration.pptx','GlobalPharmaceuticalCompany_201302_QlikView.pptx']:\n",
    "            print(i)\n",
    "    for count,i in enumerate(search_result['file_name_actual']):\n",
    "        if count<10:\n",
    "            print(i)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
