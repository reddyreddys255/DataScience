{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the right libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We get all packages that we will need during our web scraping journey\n",
    "import unicodecsv as csv #we will need this at the end of the work , to export our Data Base on CSV format\n",
    "import urllib #this library containt several modules for working with URLs:\n",
    "from urllib import request #for opening and reading URLs\n",
    "import bs4 #Beautiful Soup is a Python library for pulling data out of HTML and XML file\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd #Data Analysis Library\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and initialise the Database structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#init of the database object\n",
    "DataBase=[]\n",
    "nb_pages=0\n",
    "i=0\n",
    "#the path to your download content (to replace for yout path)\n",
    "Downdload_path=\"C:/Users/abelmhai/Desktop/learning/challenge\"\n",
    "#init objects of our Data Base \n",
    "title=''\n",
    "date=''\n",
    "abstract=''\n",
    "text_content=''\n",
    "tags=''\n",
    "external_content=''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here the first step : targeting and getting ready to walk across the data through the URL of the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#by using \"urlopen\" module from \"request\" library , we get into our website trought the URL\n",
    "start_url=request.urlopen(\"https://www.capgemini.com/resources/?search_term=&filter_content_type=client-story&filter_label=&filter_research=\")\n",
    "#Once it's done , we pull out the Data by using the \"BeautifulSoup\" module , \n",
    "#this will help us to parse it and make it easier to navigate , search or modify the source code\n",
    "start_page_content=bs4.BeautifulSoup(start_url.read())\n",
    "\n",
    "#Number of pages that we have to cross (P.S : you need to know where the NÂ° is provided in the source code)\n",
    "nb_pages= (start_page_content.find(\"div\", { \"class\" : \"pagination__current-page\" }).text).split(' ')[3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the core of the method : analyse , spot and catch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for page in range(1,int(nb_pages)+1):\n",
    "    \n",
    "    #according to which page we are , we recover the data from it\n",
    "    url_list_resources = 'https://www.capgemini.com/resources/page/'+str(page)+'/?search_term=&filter_content_type=client-story&filter_label=&filter_research='\n",
    "    page_list_resources_content=bs4.BeautifulSoup(request.urlopen(url_list_resources).read())\n",
    "    #We retrieve in a list the tags containing the explicit links to the articles of the current page\n",
    "    resources = page_list_resources_content.findAll(\"h3\", { \"class\" : \"card__media__title\" })\n",
    "    for resource in resources:\n",
    "        i+=1\n",
    "        #Across the list above , we gonna open and read each link to the articles\n",
    "        next_url=resource.findNext('a').get('href')\n",
    "        next_page_content=BeautifulSoup(request.urlopen(next_url).read())\n",
    "        #then we start getting the Data referring to our objects that we need\n",
    "        #(P.S : again , you need to study the source code to know which tag you need to catch and what information it refers )\n",
    "        title=next_page_content.find(\"h1\",{\"class\":\"component__hero-subpage__title\"}).text\n",
    "        date=next_page_content.find('time').get('datetime')\t\n",
    "        #we use here the \"try-except\" mothod to handle errors during the scrap , and avoid that our script stop running\n",
    "        try:\n",
    "            #if the variable is null all the try clause stops and goes into the except clause\n",
    "            abstract=next_page_content.find(\"div\",{\"class\":\"component__hero-inset--blog__lead\"}).text\n",
    "        except:\n",
    "            abstract=''\n",
    "            \n",
    "        #we get the text of the delivery\t\n",
    "        text_content=next_page_content.find(\"div\",{\"class\":\"article-text\"}).text \n",
    "\n",
    "        try:\n",
    "            #same process here for \"tags\"\n",
    "            tags=next_page_content.find(\"div\",{\"class\":\"component__hero-subpage__tags\"}).text\n",
    "        except:\n",
    "            tags=''\n",
    "\n",
    "        try:\n",
    "            #here we get the links to our documentations (.pdf)\n",
    "            pdf_download_link=next_page_content.find(\"div\",{\"class\":\"card__newsroom__download\"}).findNext('a').get('href')\n",
    "            #we recover the name of the file that will be used during the the export to our local storage\n",
    "            pdf_filename = pdf_download_link.split('/')[-1]\n",
    "            external_content=pdf_filename\n",
    "            #start downloading\n",
    "            request.urlretrieve(pdf_download_link,Downdload_path+'/'+pdf_filename)\n",
    "        except:\n",
    "            external_content=''\n",
    "        \n",
    "        #we build our database by containing all the objects by the right order\n",
    "        DataBase.append([title,date,abstract,text_content,tags,external_content])\n",
    "        print(\"Reference \"+ str(i) + \" scrapped\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build and export the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#We replace the \"\\n\" by an empty value because it will give line breaks when exporting the data into csv file\n",
    "for reference in DataBase:\n",
    "    for element in reference:\n",
    "        reference.replace(\"\\n\",\" \")\n",
    "\n",
    "##Exporting the database as a CSV file\n",
    "df=pd.DataFrame(DataBase,columns=['Title','Date','Abstract','Text Content','Tags','External Content'])\n",
    "df.to_csv(\"C:/Users/abelmhai/Desktop/learning/challenge/DB.csv\", sep=\";\", encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
